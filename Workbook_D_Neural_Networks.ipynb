{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d6dba1",
   "metadata": {},
   "source": [
    "## Practical I - Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7ea70",
   "metadata": {},
   "source": [
    "Welcome to the second machine learning practical. In this practical we will discover how to run an artificial neural network algorithm. A neural network, as its name implies, takes its computational form from the way neurons in a biological system work. In essence, for a given list of inputs, a neural network performs a number of processing steps before returning an output. The complexity in neural networks comes in how many of the processing steps there are, and how complex each particular step might be.\n",
    "\n",
    "This practical is fit into two sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b918d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The common tools for making neural nets in R are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ceb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"neuralnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"caret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac052cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"nnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e5267",
   "metadata": {},
   "source": [
    "A very simple example of how a neural network can work is through the use of logic gates. We use logical functions often in programming, but just as a refresher, an AND function is only true if both inputs are true. If one or both inputs are false, the result is false:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29298ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE & TRUE\n",
    "\n",
    "TRUE & FALSE\n",
    "\n",
    "FALSE & FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b25e3a",
   "metadata": {},
   "source": [
    "We can define a simple neural network as one that takes in two inputs, calculates the AND function, and gives us a result. These can be represented in graphical form where you have layers and nodes. Layers are vertical sections of the visual, and nodes are the points of computation within each layer. The mathematics of this requires the use of a bias variable, which is just a constant we add to the equation for calculation purposes and is represented as its own node, typically at the top of each layer in the neural network.\n",
    "\n",
    "In the case of the AND function, we’ll use numeric values passed into a classification function to give a value of 1 for TRUE and 0 for FALSE. We can do this using the sigmoid function. Make sure you watch the lecture for a clear explanation of the notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6880f",
   "metadata": {},
   "source": [
    "A neural network is a set of equations that we use to calculate an outcome. They aren’t so scary if we think of them as a brain made out of computer code. Depending on the number of features we have in our data, the neural network almost becomes a “black box.” In principle, we can display the equations that make up a neural network, but at a certain level, the amount of information becomes too cumbersome to intuit easily.\n",
    "\n",
    "Neural networks are used far and wide in industry largely due to their accuracy. Sometimes, there are trade-offs between having a highly accurate model, but slow computation speeds, however. Therefore, it’s best to try **multiple** models and use neural networks *only* if they work for your particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38092b",
   "metadata": {},
   "source": [
    "### Section 1 - Single-Layer Neural Networks\n",
    "In the lecture we looked at the development of an AND gate in the original artificial neuron. An AND gate follows logic like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46733c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 <- c(0, 0, 1, 1)\n",
    "x2 <- c(0, 1, 0, 1)\n",
    "logic <- data.frame(x1, x2)\n",
    "logic$AND <- as.numeric(x1 & x2)\n",
    "logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873961d7",
   "metadata": {},
   "source": [
    "If you have two 1 inputs (both TRUE), your output is 1 (TRUE). However, if either of them, or both, are 0 (FALSE), your output is also 0 (FALSE). This computation is somewhat similar to logistic regression. \n",
    "\n",
    "For the logic gate, all you need to do is pick and choose weights so that when x1 = 1 and x2 = 1, the result of z when you pass it through the sigmoid function g(z) is also 1. The way the neural network goes about computing those weights is very mathy process, but it follows the same sort of logic as used in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ebef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(neuralnet)\n",
    "\n",
    "set.seed(123)\n",
    "AND <- c(rep(0, 3), 1)\n",
    "binary.data <- data.frame(expand.grid(c(0, 1), c(0, 1)), AND)\n",
    "net <- neuralnet(AND ~ Var1 + Var2, binary.data, hidden = 0,\n",
    "    err.fct = \"ce\", linear.output = FALSE)\n",
    "plot(net, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e300e1",
   "metadata": {},
   "source": [
    "We have a number of aspects in a neural network to be aware of:\n",
    "\n",
    "#### The input layer\n",
    "This is a layer that takes in a number of features, including a bias node, which is often just an offset parameter.\n",
    "\n",
    "#### The hidden layer, or “compute” layer\n",
    "This is the layer that computes some function of each feature. The number of nodes in this hidden layer depends on the computation. Sometimes, it might be as simple as one node in this layer. Other times, the picture might be more complex with multiple hidden layers.\n",
    "\n",
    "#### The output layer\n",
    "This is a final processing node, which might be a single function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409000ec",
   "metadata": {},
   "source": [
    "Neural networks come in many different flavors, but the most popular ones stem from single or multilayered neural networks. So far, you’ve seen an example of a single-layer network, for which we take some input (1,0), process it through a sigmoid function, and get some output (0). You can, in fact, chain together these computational steps to form more interconnected and complicated models by taking the output and passing it into futher computational layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332e407",
   "metadata": {},
   "source": [
    "This code example uses the `iris` dataset that is also built in with R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "library(nnet)\n",
    "iris.nn <- nnet(Species ~ ., data = iris, size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840f542",
   "metadata": {},
   "source": [
    "This code uses the `nnet()` function with the familiar `~` operator that we’ve been using in our previous practical. The `size=2` option tells us that we are using two hidden layers for computation, which must be explicitly specified. The output that we see are iterations of the network.\n",
    "\n",
    "After the neural network has finally converged, we can use it for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table(iris$Species, predict(iris.nn, iris, type = \"class\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6960db",
   "metadata": {},
   "source": [
    "The result in the confusion matrix are the reference iris species of flowers across the top and the predicted iris species of flowers going up and down the table. So, we see the neural network performed perfectly for classifying the data of the setosa species, but missed one classification for the versicolor and virginica species, respectively. A perfect machine learning model would have zeroes for all the off-diagonal elements, but this is pretty good for an illustrative example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40b6a0",
   "metadata": {},
   "source": [
    "In R, there’s really only one neural network library that has built-in functionality for plotting neural networks. In practice, most of the time plotting neural networks is more complicated than it’s worth, as we will demonstrate later. In complex modeling scenarios, neural network diagrams and mathematics become so cumbersome that the model itself more or less becomes a trained black box. If your manager were to ask you to explain the math behind a complex neural network model, you might need to block out an entire afternoon and find the largest whiteboard in the building.\n",
    "\n",
    "The neuralnet library has built-in plotting functionality, however, and in the previous case, you are plotting the neural network that has been determined to have the lowest error in this case. The number of steps are the number of iterations that have gone on in the background to tune the particular output for its lowest error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45d805",
   "metadata": {},
   "source": [
    "### Section 2 - Multiple compute outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed3233",
   "metadata": {},
   "source": [
    "As alluded to earlier, neural networks can take multiple inputs and provide multiple outputs. If, for example, you have two functions that you want to model via neural networks, you can use R’s formula operator ~ and the + operator to simply add another response to the lefthand side of the equation during modeling,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "AND <- c(rep(0, 7), 1)\n",
    "OR <- c(0, rep(1, 7))\n",
    "binary.data <- data.frame(expand.grid(c(0, 1), c(0, 1), c(0,\n",
    "    1)), AND, OR)\n",
    "net <- neuralnet(AND + OR ~ Var1 + Var2 + Var3, binary.data,\n",
    "    hidden = 0, err.fct = \"ce\", linear.output = FALSE)\n",
    "plot(net, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399125c6",
   "metadata": {},
   "source": [
    "We can model our AND and OR functions with two equations given by the outputs in the above plot\n",
    "\n",
    "AND = 19.4 + 7.5 × Var1 + 7.6 × Var2 + 7.6 × Var3 <br>\n",
    "OR = 10.3 + 22.3 × Var1 + 21.8 × Var2 + 21.9 × Var3\n",
    "\n",
    "We can see our output in the same way as before with just one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1a2d9",
   "metadata": {},
   "source": [
    "The neural networks seem to be performing quite nicely!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae20d7",
   "metadata": {},
   "source": [
    "### Section 3 - Hidden compute layers\n",
    "So far, you have been building neural networks that have no hidden layers. That is to say, the compute layer is the same as the output layer. The neural network we computed in section 2 comprised zero layers and one output layer. Here, we will look at how adding one hidden layer of computation can help increase the model’s accuracy.\n",
    "\n",
    "Neural networks use a shorthand notation for defining their architecture, in which we note the number of input nodes, followed by a colon, the number of compute nodes in the hidden layer, another colon, and then the number of output nodes. The architecture of the neural network we built in section 2 was 3:0:1.\n",
    "\n",
    "An easier way to illustrate this is by diagramming a new neural network that has three inputs, one hidden layer, and one output layer for a 3:1:1 neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "AND <- c(rep(0, 7), 1)\n",
    "binary.data <- data.frame(expand.grid(c(0, 1), c(0, 1), c(0,\n",
    "    1)), AND, OR)\n",
    "net <- neuralnet(AND ~ Var1 + Var2 + Var3, binary.data, hidden = 1,\n",
    "    err.fct = \"ce\", linear.output = FALSE)\n",
    "plot(net, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de03f9",
   "metadata": {},
   "source": [
    "In this case, we have inserted a computation step before the output. Walking through the plot you have generated above from left to right, there are three inputs for a logic gate. These are crunched into a logistic regression function in the middle, hidden layer. The resultant equation is then pumped out to the compute layer for us to use for our AND function. The math would look something like this:\n",
    "\n",
    "H1 = 8.57 + 3.6 × Var1 – 3.5 × Var2 – 3.6 × Var3 <br>\n",
    "Which we would then pass through a logistic regression function `g(H1)`\n",
    "Next, we take that output and put it through another logistic regression node using the weights calculated on the output node:\n",
    "\n",
    "AND = 5.72 - 13.79 × g(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72548013",
   "metadata": {},
   "source": [
    "One major advantage of using a hidden layer with some hidden compute nodes is that it makes the neural network more accurate. However, the more complex you make a neural network model, the slower it will be and the more difficult it will be to simply explain it with easy-to-intuit equations. More hidden compute layers also means that you run the risk of overfitting your model, such as you’ve seen already with traditional regression modeling systems.\n",
    "\n",
    "Although the numbers tied to the weights of each compute node shown in your plot above are now becoming pretty illegible, the main takeaway here is the error and number of computation steps. In this case, the error has gone down a little bit from 0.033 to 0.027 from the last model, but you’ve also reduced the number of computational steps to get that accuracy from 143 to 61. So, not only have you increased the accuracy, but you’ve made the model computation quicker at the same time. The next plot also shows another hidden computation node added to the single hidden layer, just before the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8622d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "\n",
    "net2 <- neuralnet(AND ~ Var1 + Var2 + Var3, binary.data, hidden = 2,\n",
    "    err.fct = \"ce\", linear.output = FALSE)\n",
    "\n",
    "plot(net2, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a1152",
   "metadata": {},
   "source": [
    "Mathematically, this can be represented as two logistic regression equations being fed into a final logistic regression equation for our resultant output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9332049",
   "metadata": {},
   "source": [
    "The equations are becoming more and more complicated with each increase in the number of hidden compute nodes. The error with two nodes went up slightly from 0.29 to 0.33, but the number of iteration steps the model took to minimize that error was a little bit better in that it went down from 156 to 143. What happens if you turn the number of compute nodes even higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e84866",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "\n",
    "net4 <- neuralnet(AND ~ Var1 + Var2 + Var3, binary.data, hidden = 4,\n",
    "    err.fct = \"ce\", linear.output = FALSE)\n",
    "net8 <- neuralnet(AND ~ Var1 + Var2 + Var3, binary.data, hidden = 8,\n",
    "    err.fct = \"ce\", linear.output = FALSE)\n",
    "\n",
    "plot(net4, rep = \"best\")\n",
    "plot(net8, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3275bd4",
   "metadata": {},
   "source": [
    "The code above uses the same neural network modeling scenario, but the number of hidden computation nodes are increased first to four, and then to eight. The neural network with four hidden computation nodes had a better level of error (just slightly) than the network with only a single hidden node. The error in that case went down from 0.29 to 0.28, but the number of steps went down dramatically from 156 to 58. Quite an improvement! However, a neural network with eight hidden computation layers might have crossed into overfitting territory. In that network, error went from 0.29 to 0.34, even though the number of steps went from 156 to 51."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df38e1",
   "metadata": {},
   "source": [
    "### Section 4 - Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552bc30",
   "metadata": {},
   "source": [
    "All the neural networks thus far that we’ve played around with have had an architecture that has one input layer, one or zero hidden layers (or compute layers), and one output layer.\n",
    "\n",
    "We’ve used 1:1:1 or 1:0:1 neural networks for some classification schemes already. In those examples, we were trying to model classifications based on the AND and OR logic gate functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 <- c(0, 0, 1, 1)\n",
    "x2 <- c(0, 1, 0, 1)\n",
    "logic <- data.frame(x1, x2)\n",
    "logic$AND <- as.numeric(x1 & x2)\n",
    "logic$OR <- as.numeric(x1 | x2)\n",
    "logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030cf40",
   "metadata": {},
   "source": [
    "We can represent this table as two plots, one of which shows the input values and colors those according to the type of logic gate output we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa968d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logic$AND <- as.numeric(x1 & x2) + 1\n",
    "logic$OR <- as.numeric(x1 | x2) + 1\n",
    "\n",
    "par(mfrow = c(2, 1))\n",
    "\n",
    "plot(x = logic$x1, y = logic$x2, pch = logic$AND, cex = 2,\n",
    "    main = \"Simple Classification of Two Types\",\n",
    "    xlab = \"x\", ylab = \"y\", xlim = c(-0.5, 1.5), ylim = c(-0.5,\n",
    "        1.5))\n",
    "\n",
    "plot(x = logic$x1, y = logic$x2, pch = logic$OR, cex = 2,\n",
    "    main = \"Simple Classification of Two Types\",\n",
    "    xlab = \"x\", ylab = \"y\", xlim = c(-0.5, 1.5), ylim = c(-0.5,\n",
    "        1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9303a9",
   "metadata": {},
   "source": [
    "These plots use triangles to signify when outputs are 1 (or TRUE), and circles for which the outputs are 0 (or FALSE). In our discussion on logistic regression you will recall the separating line is called a decision boundary and had always been a straight line. However, we can’t use a straight line to try to classify more complicated logic gates like an XOR or XNOR.\n",
    "In the lecture we discussed how this problem gave the AI winter for neural networks. Let's see now how to get around that problem.\n",
    "\n",
    "In tabular form, as we’ve seen with the AND and OR functions, the XOR and XNOR functions take inputs of x1, x2, and give us a numeric output in much the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 <- c(0, 0, 1, 1)\n",
    "x2 <- c(0, 1, 0, 1)\n",
    "logic <- data.frame(x1, x2)\n",
    "logic$AND <- as.numeric(x1 & x2)\n",
    "logic$OR <- as.numeric(x1 | x2)\n",
    "logic$XOR <- as.numeric(xor(x1, x2))\n",
    "logic$XNOR <- as.numeric(x1 == x2)\n",
    "logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61159d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logic$XOR <- as.numeric(xor(x1, x2)) + 1\n",
    "logic$XNOR <- as.numeric(x1 == x2) + 1\n",
    "\n",
    "par(mfrow = c(2, 1))\n",
    "\n",
    "plot(x = logic$x1, y = logic$x2, pch = logic$XOR, cex = 2, main = \"Non-Linear Classification of Two Types\",\n",
    "    xlab = \"x\", ylab = \"y\", xlim = c(-0.5, 1.5), ylim = c(-0.5,\n",
    "        1.5))\n",
    "\n",
    "plot(x = logic$x1, y = logic$x2, pch = logic$XNOR, cex = 2, main = \"Non-Linear Classification of Two Types\",\n",
    "    xlab = \"x\", ylab = \"y\", xlim = c(-0.5, 1.5), ylim = c(-0.5,\n",
    "        1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc773f5a",
   "metadata": {},
   "source": [
    "There’s no single straight line that can separate dots on the plots generated above. If you try to plot a very simple neural network with no hidden layers for an XOR classification, the results aren’t especially gratifying. Run the cells below and see for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5128500",
   "metadata": {},
   "outputs": [],
   "source": [
    "logic$XOR <- as.numeric(xor(x1, x2))\n",
    "\n",
    "set.seed(123)\n",
    "net.xor <- neuralnet(XOR ~ x1 + x2, logic, hidden = 0, err.fct = \"ce\",\n",
    "    linear.output = FALSE)\n",
    "prediction(net.xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(net.xor, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7259d58",
   "metadata": {},
   "source": [
    "Trying to use a neural network with no hidden layers will result in a huge error. Looking at the output from the `prediction()` function, you can see that the neural network thinks that for a given scenario, such as xor(0,0), the answer is 0.5 +/- 2.77. Having an error that is much higher than the level of granularity that you’re trying to find the answer for indicates that this isn’t the best method for you to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83386843",
   "metadata": {},
   "source": [
    "Instead of the traditional approach of using one or zero hidden layers, which provide a straight line decision boundary that is being used, you must rely on nonlinear decision boundaries, or curves, to separate classes of data. By adding more hidden layers to your neural networks, you add more logistic regression decision boundaries as straight lines. From these added lines, you can draw a convex decision boundary that enables nonlinearity. For this, you must rely on a class of neural networks called **multilayer perceptrons**, or MLPs.\n",
    "\n",
    "One quick-and-dirty way of using an MLP in this case would be to use the inputs x1 and x2 to get the outputs of the AND and OR functions. You then can feed those outputs as individual inputs into a single-layer neural network,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e535a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the AND\n",
    "set.seed(123)\n",
    "and.net <- neuralnet(AND ~ x1 + x2, logic, hidden = 2, err.fct = \"ce\",\n",
    "    linear.output = FALSE)\n",
    "and.result <- data.frame(prediction(and.net)$rep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42129cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the OR \n",
    "or.net <- neuralnet(OR ~ x1 + x2, logic, hidden = 2, err.fct = \"ce\",\n",
    "    linear.output = FALSE)\n",
    "or.result <- data.frame(prediction(or.net)$rep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe270b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "as.numeric(xor(round(and.result$AND), round(or.result$OR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor.data <- data.frame(and.result$AND, or.result$OR, as.numeric(xor(round(and.result$AND),\n",
    "    round(or.result$OR))))\n",
    "names(xor.data) <- c(\"AND\", \"OR\", \"XOR\")\n",
    "\n",
    "xor.net <- neuralnet(XOR ~ AND + OR, data = xor.data, hidden = 0,\n",
    "    err.fct = \"ce\", linear.output = FALSE)\n",
    "\n",
    "prediction(xor.net)\n",
    "plot(xor.net, rep = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04492622",
   "metadata": {},
   "source": [
    "An MLP is exactly what its name implies. A perceptron is a particular type of neural network that involves a specific way of how it calculates the weights and errors, known as a feed-forward neural network. By taking that principle and adding multiple hidden layers, we make it compatible with nonlinear data like the kind we are dealing with in an XOR gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7dd27",
   "metadata": {},
   "source": [
    "### Section 5 - Neural Networks for Classification\n",
    "In a sense, we’ve already demonstrated the use of neural networks for classification via the AND and OR gates that we built in section 1. These functions take some kind of binary input and give us a binary result through logistic regression activation functions at each neural network computational node. You can think of that as single-class classification. Most of the time, we’re more interested in multiclass classification.\n",
    "\n",
    "In this case, you need to split your data into training and test sets, which is straightforward enough. Training the neural network on the training data also makes sense from our past experiences with the train/test approach to machine learning. The difference here is that when you call the `predict()` function, you do so with the `type=class` option. This helps when dealing with class data instead of numeric data that you would use with regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.df <- iris\n",
    "smp_size <- floor(0.75 * nrow(iris.df))\n",
    "\n",
    "set.seed(123)\n",
    "train_ind <- sample(seq_len(nrow(iris.df)), size = smp_size)\n",
    "\n",
    "train <- iris.df[train_ind, ]\n",
    "test <- iris.df[-train_ind, ]\n",
    "\n",
    "iris.nnet <- nnet(Species ~ ., data = train, size = 4, decay = 0.0001,\n",
    "    maxit = 500, trace = FALSE)\n",
    "predictions <- predict(iris.nnet, test[, 1:4], type = \"class\")\n",
    "table(predictions, test$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae328c",
   "metadata": {},
   "source": [
    "You can see that the confusion matrix provides a pretty good result for classification using neural networks. Look back at the example in Section 1 in this practical and the examples in the previous practical for kmeans multiclass clustering; we have no cases here that are mislabeled compared to the two mislabeled cases that we saw previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8c23a",
   "metadata": {},
   "source": [
    "### Section 6 - Classification with caret\n",
    "`caret` is a great package for machine learning in R. Classification with `caret` works in a similar manner depending on the method you are using. You can use most caret methods for classification or regression, but some are specific to one versus another. The only method that is explicitly classification only for caret is `multinom`, whereas the methods `neuralnet`, `brnn`, `qrnn`, and `mlpSGD` are explicitly regression only. You can use the rest for either classification or regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "iris.caret <- train(Species ~ ., data = train, method = \"nnet\",\n",
    "    trace = FALSE)\n",
    "predictions <- predict(iris.caret, test[, 1:4])\n",
    "table(predictions, test$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39f93a",
   "metadata": {},
   "source": [
    "The end result here is the same as earlier in terms of model accuracy, but the flexibility of caret allows you again to test against other methods pretty easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.caret.m <- train(Species ~ ., data = train, method = \"multinom\",\n",
    "    trace = FALSE)\n",
    "predictions.m <- predict(iris.caret.m, test[, 1:4])\n",
    "table(predictions.m, test$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0408cb3",
   "metadata": {},
   "source": [
    "Good to know that other methods are also quite accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c04ed",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Neural networks can seem very complicated at first glance. Often they are thought of as a black box; data goes in, and insight comes out. In reality, neural networks are pretty easy to understand in their simplest form, but difficult to explain when they become more complex. At their core, neural networks take some input values, crunch them through an activation function, and return an output. The activation function, more often than not, is usually just a sigmoid function, so you can think of neural networks as just more complicated logistic regression models. In fact, their computation with simple neural network architecture is almost identical.\n",
    "\n",
    "Neural networks become more complex when you begin changing their architecture. A neural network’s architecture is made up of an input layer, a number of hidden layers, and an output layer. The input layer is simply the values for what features you are passing in to our model. The hidden layers are those that handle the computation and processing. The output layers are the ones from which you get your results. In simple cases, neural networks can have the hidden computation layers be the same as the output layer, as in the case with modeling logic gate functions like AND and OR. An example neural architecture for a neural network with three inputs, one hidden layer, and one activation node could be 3:1:1, for example. Increasing the number of compute nodes to something like 3:8:1 tends to overfit the data.\n",
    "\n",
    "Multilayered neural networks (i.e., a 3:2:2:1 neural network) can also model nonlinear behavior. Logistic regression is good at finding decision boundaries that are straight lines to separate data into several classes or types, but it fails for nonlinear behavior. By introducing multiple decision boundaries into a system via hidden layers, you can create a curve that then can separate data, which is something that a straight line cannot do.\n",
    "\n",
    "You can use neural networks both for regression modeling and classification. However, with regression modeling, it pays to be cautious and practice data normalization. In many cases, neural networks prefer data to be in a 1 or 0 format, and trying to model data that has higher values can be problematic. For classification purposes, when you use the predict() function, you also need to pass the type='class' option in order to have the modeling behavior work appropriately.\n",
    "\n",
    "There are a slew of neural network methods that you can use with the caret function in R, as well. While some of these are limited to only regression or classification, a good majority of them are flexible enough to be used with either. It pays to be cautious in method selection not just for selecting the one that can do the job you’re interested in, but because there can be tuning or optimization parameters that might need to be passed into the model to speed it up or make it more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c399cb",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "Everything we have done in this exercise has been with random data or with the `Iris` dataset. Repeat making basic neural networks for the built in `mtcars` dataset and see how the models perform differently. You should generate both a shallow and deep neural net and compare how they perform for predicting an aspect of the dataset of your choice. Post your answer in a notebook to the Canvas discussion for week 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
